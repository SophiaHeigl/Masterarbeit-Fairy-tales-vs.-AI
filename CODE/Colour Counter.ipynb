{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6efc9bf9-6b8f-4fb9-ae55-41498dda7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0667d9b-cb2d-45ba-8842-42e2cddc8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA = {\n",
    "    \"Folk Fairy Tales\": r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\German FFT\",\n",
    "    \"GPT-5\":            r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\GPT-5\",\n",
    "    \"GPT-4o\":           r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\GPT-4o\",\n",
    "}\n",
    "\n",
    "COLOR_KEYS = [\"silber\",\"gold\",\"schwarz\",\"weiss\",\"rot\",\"blau\",\"gruen\",\"gelb\",\"pink\",\"lila\"]\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"ß\", \"ss\")\n",
    "    s = (s.replace(\"ä\",\"ae\")\n",
    "           .replace(\"ö\",\"oe\")\n",
    "           .replace(\"ü\",\"ue\"))\n",
    "    return s\n",
    "\n",
    "CANON_MAP = {\n",
    "    \"weiß\": \"weiss\", \"weiss\": \"weiss\",\n",
    "    \"grün\": \"gruen\", \"gruen\": \"gruen\",\n",
    "    \"gold\": \"gold\", \"golden\": \"gold\",\n",
    "    \"silber\": \"silber\", \"silbern\": \"silber\",\n",
    "    \"schwarz\": \"schwarz\",\n",
    "    \"rot\": \"rot\",\n",
    "    \"blau\": \"blau\",\n",
    "    \"gelb\": \"gelb\",\n",
    "    \"pink\": \"pink\",\n",
    "    \"lila\": \"lila\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a24f99-2bbd-45b0-a806-f06304b62637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sophia\\anaconda3\\envs\\Working\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'de_core_news_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig: color_counts_per_corpus_normalized.xlsx und color_counts_per_corpus_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "def to_canon(tok) -> Optional[str]:\n",
    "    lem = tok.lemma_ if tok.lemma_ else tok.text\n",
    "    ln = norm(lem)\n",
    "    key = CANON_MAP.get(lem, CANON_MAP.get(ln, ln))\n",
    "    return key if key in COLOR_KEYS else None\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"]) #small model\n",
    "\n",
    "corpus_color_counts = {c: Counter() for c in CORPORA}\n",
    "corpus_token_counts = {c: 0 for c in CORPORA}\n",
    "\n",
    "for corpus_name, corpus_dir in CORPORA.items():\n",
    "    for fname in os.listdir(corpus_dir):\n",
    "        if not fname.lower().endswith(\".txt\"):\n",
    "            continue\n",
    "        path = os.path.join(corpus_dir, fname)\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        doc = nlp(text)\n",
    "        for tok in doc:\n",
    "            if tok.is_space or tok.is_punct:\n",
    "                continue\n",
    "            corpus_token_counts[corpus_name] += 1\n",
    "            key = to_canon(tok)\n",
    "            if key is not None:\n",
    "                corpus_color_counts[corpus_name][key] += 1\n",
    "\n",
    "# tabel: counting + per_1k + %\n",
    "rows = []\n",
    "for corpus_name in CORPORA.keys():\n",
    "    total = corpus_token_counts[corpus_name]\n",
    "    row = {\"corpus\": corpus_name, \"tokens\": int(total)}\n",
    "    for k in COLOR_KEYS:\n",
    "        c = corpus_color_counts[corpus_name].get(k, 0)\n",
    "        row[f\"{k}_count\"]   = int(c)\n",
    "        row[f\"{k}_per_1k\"] = (c / total * 1000) if total else np.nan\n",
    "        row[f\"{k}_percent\"] = (c / total * 100)   if total else np.nan\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"corpus\")\n",
    "\n",
    "rate_cols = [col for col in df.columns if col.endswith(\"_per_10k\") or col.endswith(\"_percent\")]\n",
    "df[rate_cols] = df[rate_cols].round(2)\n",
    "\n",
    "df.to_excel(\"color_counts_per_corpus_normalized.xlsx\", index=False)\n",
    "df.to_csv(\"color_counts_per_corpus_normalized.csv\",\n",
    "          index=False, sep=\";\", decimal=\",\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Fertig:\",\n",
    "      \"color_counts_per_corpus_normalized.xlsx\",\n",
    "      \"und\",\n",
    "      \"color_counts_per_corpus_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "235b83ff-a547-422d-b49f-518eb5df0a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel exportiert: color_counts_per_corpus_counts_percent.xlsx\n",
      "CSV exportiert: color_counts_per_corpus_counts_percent.csv\n"
     ]
    }
   ],
   "source": [
    "# Just counts and %\n",
    "rows = []\n",
    "for corpus_name in CORPORA.keys():\n",
    "    total = corpus_token_counts.get(corpus_name, 0)\n",
    "    row = {\"corpus\": corpus_name, \"tokens\": int(total)}\n",
    "    for k in COLOR_KEYS:\n",
    "        c = int(corpus_color_counts.get(corpus_name, {}).get(k, 0))\n",
    "        row[f\"{k}_count\"] = c\n",
    "        row[f\"{k}_percent\"] = (c / total * 100) if total else np.nan\n",
    "    rows.append(row)\n",
    "\n",
    "df_counts_percent = pd.DataFrame(rows).sort_values(\"corpus\")\n",
    "\n",
    "pct_cols = [c for c in df_counts_percent.columns if c.endswith(\"_percent\")]\n",
    "df_counts_percent[pct_cols] = df_counts_percent[pct_cols].round(2)\n",
    "\n",
    "\n",
    "try:\n",
    "    import openpyxl \n",
    "    df_counts_percent.to_excel(\"color_counts_per_corpus_counts_percent.xlsx\", index=False)\n",
    "    print(\"Excel exportiert: color_counts_per_corpus_counts_percent.xlsx\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Hinweis: 'openpyxl' nicht installiert – überspringe Excel-Export.\")\n",
    "\n",
    "df_counts_percent.to_csv(\n",
    "    \"color_counts_per_corpus_counts_percent.csv\",\n",
    "    index=False, sep=\";\", decimal=\",\", encoding=\"utf-8\"\n",
    ")\n",
    "print(\"CSV exportiert: color_counts_per_corpus_counts_percent.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74045e83-ea0e-449d-8560-b99f015df4f2",
   "metadata": {},
   "source": [
    "Search for occurances of specific colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ae6435-a528-4912-ad1e-5d020bcfc664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Sätze gefunden. Export: FFT_sentences_lemma_gruen.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipe_names = nlp.pipe_names\n",
    "except NameError:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "    pipe_names = nlp.pipe_names\n",
    "\n",
    "if \"parser\" not in pipe_names and \"senter\" not in pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "def canon_key(tok):\n",
    "    lem = tok.lemma_ if tok.lemma_ else tok.text\n",
    "    lem_n = norm(lem)\n",
    "    return CANON_MAP.get(lem, CANON_MAP.get(lem_n, lem_n))\n",
    "\n",
    "FFT_DIR = CORPORA[\"Folk Fairy Tales\"]\n",
    "OUT_SENT = \"FFT_sentences_lemma_gruen.csv\"\n",
    "\n",
    "sent_rows = []\n",
    "for fname in os.listdir(FFT_DIR):\n",
    "    if not fname.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "    path = os.path.join(FFT_DIR, fname)\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for idx, sent in enumerate(doc.sents):\n",
    "        if any(not (t.is_space or t.is_punct) and canon_key(t) == \"gruen\" for t in sent):\n",
    "            sent_rows.append({\n",
    "                \"file\": fname,\n",
    "                \"sent_idx\": idx,\n",
    "                \"sentence\": sent.text.strip()\n",
    "            })\n",
    "\n",
    "df_sent = pd.DataFrame(sent_rows).sort_values([\"file\", \"sent_idx\"])\n",
    "df_sent.to_csv(OUT_SENT, index=False, encoding=\"utf-8\")\n",
    "print(f\"{len(df_sent)} Sätze gefunden. Export: {OUT_SENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b210f25-8bed-436a-85d3-71d8ca09a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Sätze gefunden. Export: FFT_sentences_lemma_gelb.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipe_names = nlp.pipe_names\n",
    "except NameError:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "    pipe_names = nlp.pipe_names\n",
    "\n",
    "if \"parser\" not in pipe_names and \"senter\" not in pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "def canon_key(tok):\n",
    "    lem = tok.lemma_ if tok.lemma_ else tok.text\n",
    "    lem_n = norm(lem)\n",
    "    return CANON_MAP.get(lem, CANON_MAP.get(lem_n, lem_n))\n",
    "\n",
    "FFT_DIR = CORPORA[\"Folk Fairy Tales\"]\n",
    "OUT_SENT = \"FFT_sentences_lemma_gelb.csv\"\n",
    "\n",
    "sent_rows = []\n",
    "for fname in os.listdir(FFT_DIR):\n",
    "    if not fname.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "    path = os.path.join(FFT_DIR, fname)\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    for idx, sent in enumerate(doc.sents):\n",
    "        if any(not (t.is_space or t.is_punct) and canon_key(t) == \"gelb\" for t in sent):\n",
    "            sent_rows.append({\n",
    "                \"file\": fname,\n",
    "                \"sent_idx\": idx,\n",
    "                \"sentence\": sent.text.strip()\n",
    "            })\n",
    "\n",
    "df_sent = pd.DataFrame(sent_rows).sort_values([\"file\", \"sent_idx\"])\n",
    "df_sent.to_csv(OUT_SENT, index=False, encoding=\"utf-8\")\n",
    "print(f\"{len(df_sent)} Sätze gefunden. Export: {OUT_SENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2501491-c2ff-4567-b024-3d4b3d6579e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Gesamt-Export: sentences_lemma_gelb_all_corpora.csv  (12 Sätze)\n",
      "   └─ Folk Fairy Tales: 7 Sätze  →  sentences_lemma_gelb_Folk_Fairy_Tales.csv\n",
      "   └─ GPT-4o: 3 Sätze  →  sentences_lemma_gelb_GPT-4o.csv\n",
      "   └─ GPT-5: 2 Sätze  →  sentences_lemma_gelb_GPT-5.csv\n",
      "\n",
      "=== Treffer je Korpus ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>n_sentences_with_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT-5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             corpus  n_sentences_with_target\n",
       "0  Folk Fairy Tales                        7\n",
       "1            GPT-4o                        3\n",
       "2             GPT-5                        2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Alle Treffer (erste 50 Zeilen) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>file</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>Bechstein_Das Dukaten-Angele_571.txt</td>\n",
       "      <td>112</td>\n",
       "      <td>Und das Angele behielt seine Tugend bei und le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>Grimm_Die drei Federn_545A.txt</td>\n",
       "      <td>32</td>\n",
       "      <td>Sie gab ihm eine ausgehöhlte gelbe Rübe mit se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>Grimm_Die drei Federn_545A.txt</td>\n",
       "      <td>36</td>\n",
       "      <td>Da griff er auf Geratewohl eine aus dem Kreise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>Grimm_Jorinde und Joringel_405.txt</td>\n",
       "      <td>28</td>\n",
       "      <td>Nun war die Sonne unter: die Eule flog in eine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>Grimm_Schneewittchen_709.txt</td>\n",
       "      <td>15</td>\n",
       "      <td>\"\\n\\nDa erschrak die Königin und ward gelb und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>Grimm_Von dem Fischer und seiner Frau_555.txt</td>\n",
       "      <td>21</td>\n",
       "      <td>Als er da nun hinkam, war die See ganz grün un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>Grimm_Von dem Fischer und seiner Frau_555.txt</td>\n",
       "      <td>44</td>\n",
       "      <td>Als er an die See kam, war das Wasser ganz vio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>ChatGPT-4o_40.txt</td>\n",
       "      <td>14</td>\n",
       "      <td>Dort, als sie die Feder zeigen wollte, kam ein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>ChatGPT-4o_80.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>Er hatte ein schmales Gesicht, durchzogen von ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>ChatGPT-4o_91.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>Eines Tages, als der Herbstwind durch das Laub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GPT-5</td>\n",
       "      <td>ChatGPT-5_52.txt</td>\n",
       "      <td>15</td>\n",
       "      <td>Er trug Früchte, doch waren sie nicht rot oder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GPT-5</td>\n",
       "      <td>ChatGPT-5_86.txt</td>\n",
       "      <td>53</td>\n",
       "      <td>Der Wolf sah sie mit seinen gelben Augen an.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              corpus                                           file  sent_idx  \\\n",
       "0   Folk Fairy Tales           Bechstein_Das Dukaten-Angele_571.txt       112   \n",
       "1   Folk Fairy Tales                 Grimm_Die drei Federn_545A.txt        32   \n",
       "2   Folk Fairy Tales                 Grimm_Die drei Federn_545A.txt        36   \n",
       "3   Folk Fairy Tales             Grimm_Jorinde und Joringel_405.txt        28   \n",
       "4   Folk Fairy Tales                   Grimm_Schneewittchen_709.txt        15   \n",
       "5   Folk Fairy Tales  Grimm_Von dem Fischer und seiner Frau_555.txt        21   \n",
       "6   Folk Fairy Tales  Grimm_Von dem Fischer und seiner Frau_555.txt        44   \n",
       "9             GPT-4o                              ChatGPT-4o_40.txt        14   \n",
       "10            GPT-4o                              ChatGPT-4o_80.txt         6   \n",
       "11            GPT-4o                              ChatGPT-4o_91.txt         3   \n",
       "7              GPT-5                               ChatGPT-5_52.txt        15   \n",
       "8              GPT-5                               ChatGPT-5_86.txt        53   \n",
       "\n",
       "                                             sentence  \n",
       "0   Und das Angele behielt seine Tugend bei und le...  \n",
       "1   Sie gab ihm eine ausgehöhlte gelbe Rübe mit se...  \n",
       "2   Da griff er auf Geratewohl eine aus dem Kreise...  \n",
       "3   Nun war die Sonne unter: die Eule flog in eine...  \n",
       "4   \"\\n\\nDa erschrak die Königin und ward gelb und...  \n",
       "5   Als er da nun hinkam, war die See ganz grün un...  \n",
       "6   Als er an die See kam, war das Wasser ganz vio...  \n",
       "9   Dort, als sie die Feder zeigen wollte, kam ein...  \n",
       "10  Er hatte ein schmales Gesicht, durchzogen von ...  \n",
       "11  Eines Tages, als der Herbstwind durch das Laub...  \n",
       "7   Er trug Früchte, doch waren sie nicht rot oder...  \n",
       "8        Der Wolf sah sie mit seinen gelben Augen an.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "TARGET_KEY = \"gelb\"\n",
    "try:\n",
    "    pipe_names = nlp.pipe_names\n",
    "except NameError:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "    pipe_names = nlp.pipe_names\n",
    "\n",
    "if \"parser\" not in pipe_names and \"senter\" not in pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\", first=True)\n",
    "\n",
    "def sentence_has_target(sent, target_key: str) -> bool:\n",
    "    for t in sent:\n",
    "        if not (t.is_space or t.is_punct):\n",
    "            if canon_key(t) == target_key:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "all_rows = []\n",
    "per_corpus_counts = {}\n",
    "\n",
    "for corpus_name, corpus_dir in CORPORA.items():\n",
    "    hit_rows = []\n",
    "    for fname in os.listdir(corpus_dir):\n",
    "        if not fname.lower().endswith(\".txt\"):\n",
    "            continue\n",
    "        path = os.path.join(corpus_dir, fname)\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {corpus_name}::{fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        doc = nlp(text)\n",
    "        for idx, sent in enumerate(doc.sents):\n",
    "            if sentence_has_target(sent, TARGET_KEY):\n",
    "                row = {\n",
    "                    \"corpus\": corpus_name,\n",
    "                    \"file\": fname,\n",
    "                    \"sent_idx\": idx,\n",
    "                    \"sentence\": sent.text.strip()\n",
    "                }\n",
    "                all_rows.append(row)\n",
    "                hit_rows.append(row)\n",
    "\n",
    "    per_corpus_counts[corpus_name] = len(hit_rows)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_all = pd.DataFrame(all_rows).sort_values([\"corpus\", \"file\", \"sent_idx\"])\n",
    "out_all = f\"sentences_lemma_{TARGET_KEY}_all_corpora.csv\"\n",
    "df_all.to_csv(out_all, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] Gesamt-Export: {out_all}  ({len(df_all)} Sätze)\")\n",
    "\n",
    "\n",
    "for corp in sorted(CORPORA.keys()):\n",
    "    sub = df_all[df_all[\"corpus\"] == corp]\n",
    "    out_c = f\"sentences_lemma_{TARGET_KEY}_{corp.replace(' ', '_')}.csv\"\n",
    "    sub.to_csv(out_c, index=False, encoding=\"utf-8\")\n",
    "    print(f\"   └─ {corp}: {len(sub)} Sätze  →  {out_c}\")\n",
    "\n",
    "from IPython.display import display\n",
    "print(\"\\n=== Treffer je Korpus ===\")\n",
    "display(pd.DataFrame(\n",
    "    [{\"corpus\": c, \"n_sentences_with_target\": n} for c, n in per_corpus_counts.items()]\n",
    ").sort_values(\"corpus\").reset_index(drop=True))\n",
    "\n",
    "print(\"\\n=== Alle Treffer (erste 50 Zeilen) ===\")\n",
    "display(df_all.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce407c-9c52-4ac5-8431-d69de6554d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
