{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0274392c-a678-4910-b28a-0c733ebdc4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import os, glob, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738efa4-0031-4a4d-902c-14308d4d00bd",
   "metadata": {},
   "source": [
    "Corpus and SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5632e-ee77-487f-b7f6-29c83d82f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA: Dict[str, str] = {\n",
    "    \"Folk Fairy Tales\": r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\German FFT\",\n",
    "    \"GPT-5\":            r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\GPT-5\",\n",
    "    \"GPT-4o\":           r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\GPT-4o\",\n",
    "}\n",
    "FILE_PATTERN = \"**/*.txt\"\n",
    "STTR_WINDOW = 1000\n",
    "OUTDIR = Path(r\"C:\\Users\\Sophia\\Downloads\\MA\\TABLES\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import spacy\n",
    "def load_de_model():\n",
    "    for name in [\"de_core_news_lg\", \"de_core_news_md\", \"de_core_news_sm\"]:\n",
    "        try:\n",
    "            return spacy.load(name, disable=[\"ner\",\"lemmatizer\",\"textcat\",\"morphologizer\",\"tagger\",\"attribute_ruler\"])\n",
    "        except Exception:\n",
    "            continue\n",
    "    nlp = spacy.blank(\"de\")\n",
    "    if \"senter\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"sentencizer\")\n",
    "    return nlp\n",
    "\n",
    "nlp = load_de_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9b68b6-610d-480d-80c1-95b58427f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sophia\\anaconda3\\envs\\Working\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'de_core_news_lg' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "Processing Folk Fairy Tales: 100%|██████████| 122/122 [00:23<00:00,  5.15file/s]\n",
      "Processing GPT-5: 100%|██████████| 100/100 [00:15<00:00,  6.63file/s]\n",
      "Processing GPT-4o: 100%|██████████| 100/100 [00:13<00:00,  7.31file/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author/Group</th>\n",
       "      <th>N_docs</th>\n",
       "      <th>Mean word length (chars) (mean ± sd)</th>\n",
       "      <th>Word length variation (sd)</th>\n",
       "      <th>Mean sentence length (words) (mean ± sd)</th>\n",
       "      <th>Sentence length variation (sd)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overall mean</td>\n",
       "      <td>322.0</td>\n",
       "      <td>4.58 ± 0.13</td>\n",
       "      <td>2.121170</td>\n",
       "      <td>16.62 ± 3.55</td>\n",
       "      <td>8.756184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>122.0</td>\n",
       "      <td>4.62 ± 0.19</td>\n",
       "      <td>2.247374</td>\n",
       "      <td>20.33 ± 2.72</td>\n",
       "      <td>11.267519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.54 ± 0.08</td>\n",
       "      <td>2.008610</td>\n",
       "      <td>14.03 ± 1.53</td>\n",
       "      <td>7.538948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT-5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.59 ± 0.08</td>\n",
       "      <td>2.079760</td>\n",
       "      <td>14.68 ± 1.36</td>\n",
       "      <td>6.909592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author/Group  N_docs Mean word length (chars) (mean ± sd)  \\\n",
       "3      Overall mean   322.0                          4.58 ± 0.13   \n",
       "0  Folk Fairy Tales   122.0                          4.62 ± 0.19   \n",
       "1            GPT-4o   100.0                          4.54 ± 0.08   \n",
       "2             GPT-5   100.0                          4.59 ± 0.08   \n",
       "\n",
       "   Word length variation (sd) Mean sentence length (words) (mean ± sd)  \\\n",
       "3                    2.121170                             16.62 ± 3.55   \n",
       "0                    2.247374                             20.33 ± 2.72   \n",
       "1                    2.008610                             14.03 ± 1.53   \n",
       "2                    2.079760                             14.68 ± 1.36   \n",
       "\n",
       "   Sentence length variation (sd)  \n",
       "3                        8.756184  \n",
       "0                       11.267519  \n",
       "1                        7.538948  \n",
       "2                        6.909592  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"senter\" not in nlp.pipe_names and \"parser\" not in nlp.pipe_names:\n",
    "    try:\n",
    "        nlp.add_pipe(\"sentencizer\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def read_text(fp: str) -> str:\n",
    "    try:\n",
    "        return Path(fp).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        with open(fp, \"r\", encoding=\"latin-1\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "\n",
    "def words_from_doc(doc) -> List[str]:\n",
    "    return [t.text for t in doc if t.is_alpha]\n",
    "\n",
    "def sentence_word_counts(doc) -> List[int]:\n",
    "    counts = []\n",
    "    for sent in doc.sents:\n",
    "        c = sum(1 for t in sent if t.is_alpha)\n",
    "        if c > 0:\n",
    "            counts.append(c)\n",
    "    return counts\n",
    "\n",
    "def mean_and_sd(arr: List[float]) -> Tuple[float, float]:\n",
    "    if not arr:\n",
    "        return 0.0, 0.0\n",
    "    a = np.array(arr, dtype=float)\n",
    "    return float(a.mean()), float(a.std(ddof=1)) if len(a) > 1 else (float(a.mean()), 0.0)\n",
    "\n",
    "def sttr_percent(tokens: List[str], window: int = 1000) -> float:\n",
    "    n = len(tokens)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    if n <= window:\n",
    "        types = len(set(t.lower() for t in tokens))\n",
    "        return 100.0 * types / max(1, n)r)\n",
    "    blocks = [tokens[i:i+window] for i in range(0, n, window)]\n",
    "    vals = []\n",
    "    for b in blocks:\n",
    "        if not b:\n",
    "            continue\n",
    "        vals.append(len(set(t.lower() for t in b)) / len(b))\n",
    "    return 100.0 * (sum(vals) / len(vals)) if vals else 0.0\n",
    "\n",
    "# doc metrics\n",
    "rows_doc = []\n",
    "for group, cdir in CORPORA.items():\n",
    "    files = glob.glob(os.path.join(cdir, FILE_PATTERN), recursive=True)\n",
    "    for fp in tqdm(files, desc=f\"Processing {group}\", unit=\"file\"):\n",
    "        txt = read_text(fp)\n",
    "        if not txt.strip():\n",
    "            continue\n",
    "        doc = nlp(txt)\n",
    "n\n",
    "        words = words_from_doc(doc)\n",
    "        word_lengths = [len(w) for w in words]\n",
    "\n",
    "        sent_counts = sentence_word_counts(doc)\n",
    "\n",
    "        # Tokens/Types/stTTR \n",
    "        n_tokens = len(words)\n",
    "        n_types  = len(set(w.lower() for w in words))\n",
    "        sttr_val = sttr_percent(words, window=STTR_WINDOW)\n",
    "\n",
    "        # mean & SD within doc\n",
    "        mean_wlen, sd_wlen   = mean_and_sd(word_lengths)\n",
    "        mean_slen, sd_slen   = mean_and_sd(sent_counts)\n",
    "\n",
    "        rows_doc.append({\n",
    "            \"group\": group,\n",
    "            \"path\": fp,\n",
    "            \"n_tokens\": n_tokens,\n",
    "            \"n_types\": n_types,\n",
    "            \"sttr_percent\": sttr_val,\n",
    "            \"mean_word_len\": mean_wlen,\n",
    "            \"sd_word_len\": sd_wlen,           \n",
    "            \"mean_sent_len\": mean_slen,\n",
    "            \"sd_sent_len\": sd_slen,            \n",
    "        })\n",
    "\n",
    "df_docs = pd.DataFrame(rows_doc)\n",
    "df_docs.to_csv(OUTDIR / \"per_document_metrics.csv\", index=False)\n",
    "\n",
    "\n",
    "def agg_complexity_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def summarize(group_df: pd.DataFrame) -> pd.Series:\n",
    "        n_docs = len(group_df)\n",
    "        mw_mean, mw_sd = group_df[\"mean_word_len\"].mean(), group_df[\"mean_word_len\"].std(ddof=1)\n",
    "        ms_mean, ms_sd = group_df[\"mean_sent_len\"].mean(), group_df[\"mean_sent_len\"].std(ddof=1)\n",
    "        wl_var = group_df[\"sd_word_len\"].mean()     \n",
    "        sl_var = group_df[\"sd_sent_len\"].mean()    \n",
    "        return pd.Series({\n",
    "            \"N_docs\": n_docs,\n",
    "            \"Mean word length (mean)\": mw_mean,\n",
    "            \"Mean word length (sd)\": mw_sd if n_docs > 1 else 0.0,\n",
    "            \"Word length variation (sd)\": wl_var,\n",
    "            \"Mean sentence length (mean)\": ms_mean,\n",
    "            \"Mean sentence length (sd)\": ms_sd if n_docs > 1 else 0.0,\n",
    "            \"Sentence length variation (sd)\": sl_var,\n",
    "        })\n",
    "\n",
    "    parts = []\n",
    "    # group\n",
    "    parts.append(df.groupby(\"group\").apply(summarize).reset_index().rename(columns={\"group\":\"Author/Group\"}))\n",
    "    # overall\n",
    "    overall = summarize(df)\n",
    "    overall_df = pd.DataFrame([overall])\n",
    "    overall_df.insert(0, \"Author/Group\", \"Overall mean\")\n",
    "    parts.append(overall_df)\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    out[\"Mean word length (chars) (mean ± sd)\"] = out.apply(\n",
    "        lambda r: f\"{r['Mean word length (mean)']:.2f} ± {r['Mean word length (sd)']:.2f}\", axis=1)\n",
    "    out[\"Mean sentence length (words) (mean ± sd)\"] = out.apply(\n",
    "        lambda r: f\"{r['Mean sentence length (mean)']:.2f} ± {r['Mean sentence length (sd)']:.2f}\", axis=1)\n",
    "\n",
    "    nice_cols = [\n",
    "        \"Author/Group\", \"N_docs\",\n",
    "        \"Mean word length (chars) (mean ± sd)\",\n",
    "        \"Word length variation (sd)\",\n",
    "        \"Mean sentence length (words) (mean ± sd)\",\n",
    "        \"Sentence length variation (sd)\"\n",
    "    ]\n",
    "    order = [\"Overall mean\", \"Folk Fairy Tales\", \"GPT-4o\", \"GPT-5\"]\n",
    "    out[\"__order__\"] = out[\"Author/Group\"].apply(lambda x: order.index(x) if x in order else 999)\n",
    "    out = out.sort_values(\"__order__\").drop(columns=[\"__order__\"])\n",
    "    return out[nice_cols]\n",
    "\n",
    "table1 = agg_complexity_table(df_docs)\n",
    "table1.to_csv(OUTDIR / \"table_sentence_word_complexity.csv\", index=False)\n",
    "table1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a692f-9a6b-4ce7-9b65-a1248f32c886",
   "metadata": {},
   "source": [
    "Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2b55a7-2679-4fa7-ae3a-6936382792c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author/Group</th>\n",
       "      <th>N_docs</th>\n",
       "      <th>Tokens (mean ± sd)</th>\n",
       "      <th>Types (mean ± sd)</th>\n",
       "      <th>stTTR % (mean ± sd)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Overall mean</td>\n",
       "      <td>322.0</td>\n",
       "      <td>1367.63 ± 622.34</td>\n",
       "      <td>524.07 ± 136.86</td>\n",
       "      <td>53.12 ± 7.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Folk Fairy Tales</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1635.79 ± 937.51</td>\n",
       "      <td>555.03 ± 210.86</td>\n",
       "      <td>47.36 ± 6.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1146.79 ± 99.41</td>\n",
       "      <td>483.77 ± 32.68</td>\n",
       "      <td>57.87 ± 5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT-5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1261.31 ± 149.65</td>\n",
       "      <td>526.60 ± 49.93</td>\n",
       "      <td>55.39 ± 4.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author/Group  N_docs Tokens (mean ± sd) Types (mean ± sd)  \\\n",
       "3      Overall mean   322.0   1367.63 ± 622.34   524.07 ± 136.86   \n",
       "0  Folk Fairy Tales   122.0   1635.79 ± 937.51   555.03 ± 210.86   \n",
       "1            GPT-4o   100.0    1146.79 ± 99.41    483.77 ± 32.68   \n",
       "2             GPT-5   100.0   1261.31 ± 149.65    526.60 ± 49.93   \n",
       "\n",
       "  stTTR % (mean ± sd)  \n",
       "3        53.12 ± 7.61  \n",
       "0        47.36 ± 6.94  \n",
       "1        57.87 ± 5.95  \n",
       "2        55.39 ± 4.98  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUTDIR = Path(r\"C:\\Users\\Sophia\\Downloads\\MA\\TABLES\")\n",
    "df_docs = pd.read_csv(OUTDIR / \"per_document_metrics.csv\")\n",
    "\n",
    "def agg_lexdiv_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def summarize(group_df: pd.DataFrame) -> pd.Series:\n",
    "        n_docs = len(group_df)\n",
    "        tok_mean, tok_sd   = group_df[\"n_tokens\"].mean(), group_df[\"n_tokens\"].std(ddof=1)\n",
    "        typ_mean, typ_sd   = group_df[\"n_types\"].mean(),  group_df[\"n_types\"].std(ddof=1)\n",
    "        sttr_mean, sttr_sd = group_df[\"sttr_percent\"].mean(), group_df[\"sttr_percent\"].std(ddof=1)\n",
    "        return pd.Series({\n",
    "            \"N_docs\": n_docs,\n",
    "            \"Tokens (mean)\": tok_mean, \"Tokens (sd)\": tok_sd if n_docs > 1 else 0.0,\n",
    "            \"Types (mean)\": typ_mean, \"Types (sd)\": typ_sd if n_docs > 1 else 0.0,\n",
    "            \"stTTR % (mean)\": sttr_mean, \"stTTR % (sd)\": sttr_sd if n_docs > 1 else 0.0,\n",
    "        })\n",
    "\n",
    "    parts = []\n",
    "    parts.append(df.groupby(\"group\").apply(summarize).reset_index().rename(columns={\"group\":\"Author/Group\"}))\n",
    "    overall = summarize(df)\n",
    "    overall_df = pd.DataFrame([overall])\n",
    "    overall_df.insert(0, \"Author/Group\", \"Overall mean\")\n",
    "    parts.append(overall_df)\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    out[\"Tokens (mean ± sd)\"] = out.apply(lambda r: f\"{r['Tokens (mean)']:.2f} ± {r['Tokens (sd)']:.2f}\", axis=1)\n",
    "    out[\"Types (mean ± sd)\"]  = out.apply(lambda r: f\"{r['Types (mean)']:.2f} ± {r['Types (sd)']:.2f}\", axis=1)\n",
    "    out[\"stTTR % (mean ± sd)\"]= out.apply(lambda r: f\"{r['stTTR % (mean)']:.2f} ± {r['stTTR % (sd)']:.2f}\", axis=1)\n",
    "\n",
    "    nice_cols = [\"Author/Group\", \"N_docs\", \"Tokens (mean ± sd)\", \"Types (mean ± sd)\", \"stTTR % (mean ± sd)\"]\n",
    "    order = [\"Overall mean\", \"Folk Fairy Tales\", \"GPT-4o\", \"GPT-5\"]\n",
    "    out[\"__order__\"] = out[\"Author/Group\"].apply(lambda x: order.index(x) if x in order else 999)\n",
    "    out = out.sort_values(\"__order__\").drop(columns=[\"__order__\"])\n",
    "    return out[nice_cols]\n",
    "\n",
    "table2 = agg_lexdiv_table(df_docs)\n",
    "table2.to_csv(OUTDIR / \"table_lexical_diversity.csv\", index=False)\n",
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b975f6f6-2621-4ce1-b224-1d2d53b20a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
