{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7187102-e93a-40a1-9f55-2101d8975f1a",
   "metadata": {},
   "source": [
    "1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9049d0d-e871-4920-b7bf-84508712518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a17bb-f0b5-4913-8cfb-920b0ca8e342",
   "metadata": {},
   "source": [
    "2. Corpora | Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d15e0-5e5f-4bfb-99bb-15c15ac2fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA = {\n",
    "    \"Folk Fairy Tales\": r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\German FFT\",\n",
    "    \"GPT-5\":            r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\GPT-5\",\n",
    "    \"GPT-4o\":           r\"C:\\Users\\Sophia\\Downloads\\MA\\CORPORA\\GPT-4o\",\n",
    "}\n",
    "SWS_DIR = Path(r\"C:\\Users\\Sophia\\Downloads\\MA\\CODE\\Sentiment Analyse\\SentiWS_v2.0\")\n",
    "\n",
    "OUTDIR = Path(r\"C:\\Users\\Sophia\\Downloads\\MA\\TABLES\\Sentiment\\SentiWS\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOP_K = 5\n",
    "POS_THRESH, NEG_THRESH = 0.05, -0.05\n",
    "CONF_POS,  CONF_NEG    = 0.60, -0.60\n",
    "\n",
    "CSV_KWARGS = dict(index=False, sep=\";\", encoding=\"utf-8\", float_format=\"%.6f\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4eae1c-e663-4463-997c-f3718fc5ea60",
   "metadata": {},
   "source": [
    "3. Loading SentiWS (Remus et al. 2010) https://wortschatz.uni-leipzig.de/en/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51573c-8c67-44de-9855-5f753fd2e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiws(base: Path) -> dict:\n",
    "    lex = {}\n",
    "    num_re = re.compile(r'[-+]?\\d+(?:[.,]\\d+)?')\n",
    "    for p in base.rglob(\"*.txt\"):\n",
    "        try:\n",
    "            raw = p.read_text(encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            raw = p.read_text(encoding=\"latin-1\", errors=\"ignore\")\n",
    "        for line in raw.splitlines():\n",
    "            line = line.lstrip(\"\\ufeff\").strip()\n",
    "            if not line or line.startswith((\"%\", \"#\")):\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                parts = re.split(r\"\\s+\", line, maxsplit=2)\n",
    "            head = parts[0] if parts else \"\"\n",
    "            score = None\n",
    "            if len(parts) >= 2:\n",
    "                s = parts[1].strip().replace(\",\", \".\")\n",
    "                try:\n",
    "                    score = float(s)\n",
    "                except ValueError:\n",
    "                    score = None\n",
    "            if score is None:\n",
    "                m = num_re.search(line)\n",
    "                if not m:\n",
    "                    continue\n",
    "                score = float(m.group(0).replace(\",\", \".\"))\n",
    "            if \"#\" in head:\n",
    "                lemma = head.split(\"#\", 1)[0]\n",
    "            elif \"|\" in head:\n",
    "                lemma = head.split(\"|\", 1)[0]\n",
    "            else:\n",
    "                lemma = head\n",
    "            tokens = [lemma]\n",
    "            if len(parts) >= 3 and parts[2].strip():\n",
    "                forms_str = parts[2]\n",
    "                forms = [f.strip() for f in re.split(r\"[;,]\", forms_str) if f.strip()]\n",
    "                tokens.extend(forms)\n",
    "            for t in tokens:\n",
    "                t_low = t.lower()\n",
    "                prev = lex.get(t_low)\n",
    "                if prev is None or abs(score) > abs(prev):\n",
    "                    lex[t_low] = score\n",
    "                if \"ß\" in t_low:\n",
    "                    t_ss = t_low.replace(\"ß\", \"ss\")\n",
    "                    prev = lex.get(t_ss)\n",
    "                    if prev is None or abs(score) > abs(prev):\n",
    "                        lex[t_ss] = score\n",
    "    return lex\n",
    "\n",
    "LEX = load_sentiws(SWS_DIR)\n",
    "print(f\"[INFO] SentiWS geladen: {len(LEX):,} Einträge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3234ca-9629-4ba2-92e7-f6fd5dad017a",
   "metadata": {},
   "source": [
    "Valence Shifters and Normalisation for historic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d3652-94a9-4894-acc4-e9cea6cbcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIONS = {\n",
    "    \"nicht\",\"kein\",\"keine\",\"keiner\",\"keines\",\"keinem\",\"keinen\",\n",
    "    \"nie\",\"niemals\",\"nimmer\",\"nimmermehr\",\"ohne\",\"ohn\",\"nichts\",\"weder\",\"keineswegs\"\n",
    "}\n",
    "INTENSIFIERS = {\n",
    "    \"sehr\":1.5, \"äußerst\":1.7, \"aeusserst\":1.7, \"überaus\":1.7, \"ueberaus\":1.7,\n",
    "    \"wirklich\":1.3, \"gar\":1.2, \"besonders\":1.3, \"hoechst\":1.6, \"höchst\":1.6\n",
    "}\n",
    "DIMINISHERS = {\n",
    "    \"kaum\":0.5, \"wenig\":0.7, \"ein wenig\":0.7, \"ein bisschen\":0.7, \"bisschen\":0.7,\n",
    "    \"wenigstens\":0.85\n",
    "}\n",
    "WINDOW = 3\n",
    "\n",
    "TH_WORD_MAP = {\n",
    "    \"thun\":\"tun\",\"gethan\":\"getan\",\"gethanen\":\"getanen\",\"thäte\":\"täte\",\"thät\":\"tät\",\"thut\":\"tut\",\"that\":\"tat\",\"thaten\":\"taten\",\n",
    "    \"thal\":\"tal\",\"thale\":\"tale\",\"thales\":\"tales\",\"thaler\":\"taler\",\n",
    "    \"thor\":\"tor\",\"thore\":\"tore\",\"thoren\":\"toren\",\"thorheit\":\"torheit\",\n",
    "    \"thier\":\"tier\",\"thiere\":\"tiere\",\"thieren\":\"tieren\",\"thräne\":\"träne\",\"thränen\":\"tränen\",\n",
    "}\n",
    "SEYN_MAP = {\n",
    "    \"seyn\":\"sein\",\"sey\":\"sei\",\"seyd\":\"seid\",\"seyin\":\"sein\",\"seyet\":\"seiet\",\"seyest\":\"seiest\",\"seyende\":\"seiende\",\"seyender\":\"seiender\",\n",
    "}\n",
    "def hist_normalize_token(tok: str) -> str:\n",
    "    s = tok.lower()\n",
    "    s = s.replace(\"ſ\",\"s\")\n",
    "    if s in TH_WORD_MAP: s = TH_WORD_MAP[s]\n",
    "    if s in SEYN_MAP:    s = SEYN_MAP[s]\n",
    "    s = (s.replace(\"ä\",\"ae\").replace(\"ö\",\"oe\").replace(\"ü\",\"ue\"))\n",
    "    s = s.replace(\"ß\",\"ss\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbd41e-a833-4b8b-8591-3b45d47527c5",
   "metadata": {},
   "source": [
    "TABLES | FIGURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25aa73-647a-4fe4-b05b-a38419842df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\", exclude=[\"parser\",\"ner\",\"textcat\"])\n",
    "if \"senter\" not in nlp.pipe_names and \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "print(\"[INFO] spaCy pipeline:\", nlp.pipe_names)\n",
    "\n",
    "\n",
    "contrib_by_corpus = defaultdict(Counter)    \n",
    "contrib_overall = Counter()\n",
    "contrib_pos_by_corpus = defaultdict(Counter) \n",
    "contrib_neg_by_corpus = defaultdict(Counter)  )\n",
    "contrib_pos_overall = Counter()\n",
    "contrib_neg_overall = Counter()\n",
    "\n",
    "=\n",
    "def sentiws_sentence_scores(\n",
    "    sent_doc,\n",
    "    contrib_counter_abs: Counter=None,\n",
    "    corpus_key: str=None,\n",
    "    contrib_pos_ctr: Counter=None,\n",
    "    contrib_neg_ctr: Counter=None\n",
    ") -> dict:\n",
    "    toks = [t for t in sent_doc if t.is_alpha]\n",
    "    scores = []\n",
    "    lowers = [t.lower_ for t in toks]\n",
    "    lemmas = [t.lemma_.lower() for t in toks]\n",
    "\n",
    "    for i, t in enumerate(toks):\n",
    "        key_lemma = hist_normalize_token(lemmas[i])\n",
    "        key_form  = hist_normalize_token(lowers[i])\n",
    "\n",
    "        sc = LEX.get(key_lemma)\n",
    "        if sc is None: sc = LEX.get(key_form)\n",
    "        if sc is None: continue\n",
    "\n",
    "        start = max(0, i - WINDOW)\n",
    "        context_lowers = [hist_normalize_token(x) for x in lowers[start:i]]\n",
    "        context_lemmas = [hist_normalize_token(x) for x in lemmas[start:i]]\n",
    "        context = context_lemmas[::-1]\n",
    "\n",
    "        mult = 1.0; negate = False\n",
    "        context_surface = \" \".join(context_lowers)\n",
    "        if \"ein bisschen\" in context_surface: mult *= DIMINISHERS[\"ein bisschen\"]\n",
    "        if \"ein wenig\"   in context_surface: mult *= DIMINISHERS[\"ein wenig\"]\n",
    "        for w in context:\n",
    "            if w in NEGATIONS: negate = True; break\n",
    "            if w in INTENSIFIERS: mult *= INTENSIFIERS[w]\n",
    "            if w in DIMINISHERS:  mult *= DIMINISHERS[w]\n",
    "\n",
    "        val = sc * mult\n",
    "        if negate: val = -val\n",
    "        scores.append(val)\n",
    "\n",
    "        token_key = key_lemma if key_lemma in LEX else key_form\n",
    "        if contrib_counter_abs is not None:\n",
    "            contrib_counter_abs[token_key] += abs(val)\n",
    "        if val > 0 and contrib_pos_ctr is not None:\n",
    "            contrib_pos_ctr[token_key] += val\n",
    "        elif val < 0 and contrib_neg_ctr is not None:\n",
    "            contrib_neg_ctr[token_key] += (-val)\n",
    "\n",
    "    if not scores:\n",
    "        return {\"compound\": 0.0, \"raw_sum\": 0.0, \"n_polar\": 0, \"label\": \"neutral\"}\n",
    "\n",
    "    mean_score = float(np.mean(scores))\n",
    "    compound = float(np.tanh(mean_score))\n",
    "    label = \"positive\" if compound >= POS_THRESH else \"negative\" if compound <= NEG_THRESH else \"neutral\"\n",
    "    return {\"compound\": compound, \"raw_sum\": float(np.sum(scores)), \"n_polar\": len(scores), \"label\": label}\n",
    "\n",
    "def read_text_safe(fp: Path) -> str:\n",
    "    for enc in (\"utf-8\", \"latin-1\"):\n",
    "        try:\n",
    "            return fp.read_text(encoding=enc, errors=\"ignore\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1581a-0f1f-45ba-9fb5-6b8b6e7d89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_sent, rows_doc = [], []\n",
    "for corpus_name, corpus_dir in CORPORA.items():\n",
    "    files = list(Path(corpus_dir).rglob(\"*.txt\"))\n",
    "    print(f\"[INFO] {corpus_name}: {len(files)} Dateien\")\n",
    "    for fp in tqdm(files, desc=f\"SentiWS Sätze {corpus_name}\", unit=\"Datei\"):\n",
    "        text = read_text_safe(Path(fp))\n",
    "        if not text.strip(): continue\n",
    "        doc = nlp(text)\n",
    "        sents = [s for s in doc.sents if s.text.strip()]\n",
    "        if not sents: continue\n",
    "\n",
    "        comp = []\n",
    "        for i, s in enumerate(sents):\n",
    "            sc = sentiws_sentence_scores(\n",
    "                s,\n",
    "                contrib_counter_abs=contrib_by_corpus[corpus_name],\n",
    "                corpus_key=corpus_name,\n",
    "                contrib_pos_ctr=contrib_pos_by_corpus[corpus_name],\n",
    "                contrib_neg_ctr=contrib_neg_by_corpus[corpus_name],\n",
    "            )\n",
    "            _ = sentiws_sentence_scores(\n",
    "                s,\n",
    "                contrib_counter_abs=contrib_overall,\n",
    "                corpus_key=\"__ALL__\",\n",
    "                contrib_pos_ctr=contrib_pos_overall,\n",
    "                contrib_neg_ctr=contrib_neg_overall,\n",
    "            )\n",
    "            comp.append(sc[\"compound\"])\n",
    "            rows_sent.append({\n",
    "                \"corpus\": corpus_name, \"file\": str(fp), \"sent_idx\": i,\n",
    "                \"sent_text\": s.text.strip(),\n",
    "                \"sws_compound\": sc[\"compound\"],\n",
    "                \"sws_raw_sum\": sc[\"raw_sum\"],\n",
    "                \"sws_n_polar\": sc[\"n_polar\"],\n",
    "                \"sws_label\": sc[\"label\"],\n",
    "            })\n",
    "\n",
    "        c = np.array(comp, dtype=float)\n",
    "        if c.size == 0: continue\n",
    "        labels = np.array([\"positive\" if x>=POS_THRESH else \"negative\" if x<=NEG_THRESH else \"neutral\" for x in c])\n",
    "        share_pos = float((labels==\"positive\").mean()) * 100.0\n",
    "        share_neg = float((labels==\"negative\").mean()) * 100.0\n",
    "        conf_pos  = float((c >= CONF_POS).mean()) * 100.0\n",
    "        conf_neg  = float((c <= CONF_NEG).mean()) * 100.0\n",
    "        top_pos_idx = np.argsort(-c)[:TOP_K]\n",
    "        top_neg_idx = np.argsort(c)[:TOP_K]\n",
    "\n",
    "        row = {\n",
    "            \"corpus\": corpus_name, \"file\": str(fp),\n",
    "            \"n_sents\": int(len(sents)),\n",
    "            \"sws_label_pos_percent\": share_pos,\n",
    "            \"sws_label_neg_percent\": share_neg,\n",
    "            \"sws_conf_pos_percent\":  conf_pos,\n",
    "            \"sws_conf_neg_percent\":  conf_neg,\n",
    "        }\n",
    "        for j, si in enumerate(top_pos_idx, start=1):\n",
    "            row[f\"top_pos_{j}_compound\"] = float(c[si])\n",
    "            row[f\"top_pos_{j}_sent\"]     = sents[si].text.strip()\n",
    "        for j, si in enumerate(top_neg_idx, start=1):\n",
    "            row[f\"top_neg_{j}_compound\"] = float(c[si])\n",
    "            row[f\"top_neg_{j}_sent\"]     = sents[si].text.strip()\n",
    "        rows_doc.append(row)\n",
    "\n",
    "df_sent = pd.DataFrame(rows_sent)\n",
    "df_doc  = pd.DataFrame(rows_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b710c-faa6-4860-9122-d865ee00af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_csv = OUTDIR / \"sentiws_sentence_scores_long.csv\"\n",
    "doc_csv  = OUTDIR / \"sentiws_doc_aggregates.csv\"\n",
    "df_sent.to_csv(sent_csv, **CSV_KWARGS)\n",
    "df_doc.to_csv(doc_csv, **CSV_KWARGS)\n",
    "\n",
    "agg_cols = [\"sws_label_pos_percent\",\"sws_label_neg_percent\",\"sws_conf_pos_percent\",\"sws_conf_neg_percent\"]\n",
    "summary = df_doc.groupby(\"corpus\")[agg_cols].mean().reset_index()\n",
    "sum_csv = OUTDIR / \"sentiws_corpus_summary.csv\"\n",
    "summary.to_csv(sum_csv, **CSV_KWARGS)\n",
    "\n",
    "corpus_order = [c for c in CORPORA.keys() if c in df_doc[\"corpus\"].unique()]\n",
    "grp = df_doc.groupby(\"corpus\")\n",
    "mean_pos = grp[\"sws_label_pos_percent\"].mean()\n",
    "mean_neg = grp[\"sws_label_neg_percent\"].mean()\n",
    "mean_neu = 100.0 - mean_pos - mean_neg\n",
    "x = np.arange(len(corpus_order))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(x, [mean_pos.get(c,0.0) for c in corpus_order], label=\"positive\")\n",
    "plt.bar(x, [mean_neu.get(c,0.0) for c in corpus_order], bottom=[mean_pos.get(c,0.0) for c in corpus_order], label=\"neutral\")\n",
    "plt.bar(x, [mean_neg.get(c,0.0) for c in corpus_order], bottom=[mean_pos.get(c,0.0)+mean_neu.get(c,0.0) for c in corpus_order], label=\"negative\")\n",
    "plt.xticks(x, corpus_order, rotation=0)\n",
    "plt.ylabel(\"% sentences\")\n",
    "plt.title(\"SentiWS: average sentence-label distribution by corpus\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "f1 = OUTDIR / \"viz_sentiws_label_share_stacked_by_corpus.png\"\n",
    "plt.savefig(f1, dpi=200); plt.show(); print(f\"[OK] Plot:\", f1)\n",
    "\n",
    "for col, title, fname in [\n",
    "    (\"sws_label_pos_percent\", \"SentiWS: distribution % positive sentences\", \"viz_sentiws_pos_boxplot_by_corpus.png\"),\n",
    "    (\"sws_label_neg_percent\", \"SentiWS: distribution % negative sentences\", \"viz_sentiws_neg_boxplot_by_corpus.png\"),\n",
    "]:\n",
    "    box_data = [df_doc.loc[df_doc[\"corpus\"]==c, col].dropna().values for c in corpus_order]\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.boxplot(box_data, labels=corpus_order, showfliers=False)\n",
    "    plt.ylabel(\"% sentences (doc)\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    fp = OUTDIR / fname\n",
    "    plt.savefig(fp, dpi=200); plt.show(); print(f\"[OK] Plot:\", fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc363c04-ff8c-47fa-8aa1-55b73094a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_wide(counter_by_corpus: dict, k: int = 10) -> pd.DataFrame:\n",
    "    cols = []\n",
    "    data = {}\n",
    "    for corp, ctr in counter_by_corpus.items():\n",
    "        items = sorted(ctr.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        tokens = [tok for tok, _ in items[:k]]\n",
    "        if len(tokens) < k:\n",
    "            tokens += [\"\"] * (k - len(tokens))\n",
    "        data[corp] = tokens\n",
    "        cols.append(corp)\n",
    "    df = pd.DataFrame(data, index=[i for i in range(1, k+1)])\n",
    "    ordered_cols = [c for c in CORPORA.keys() if c in df.columns]\n",
    "    return df[ordered_cols]\n",
    "\n",
    "df_pos_wide = topk_wide(contrib_pos_by_corpus, k=10)\n",
    "df_neg_wide = topk_wide(contrib_neg_by_corpus, k=10)\n",
    "\n",
    "pos_wide_csv = OUTDIR / \"top10_positive_by_corpus_wide.csv\"\n",
    "neg_wide_csv = OUTDIR / \"top10_negative_by_corpus_wide.csv\"\n",
    "df_pos_wide.to_csv(pos_wide_csv, sep=\";\", encoding=\"utf-8\")\n",
    "df_neg_wide.to_csv(neg_wide_csv, sep=\";\", encoding=\"utf-8\")\n",
    "print(\"[OK] Top-10 positive (wide):\", pos_wide_csv)\n",
    "print(\"[OK] Top-10 negative (wide):\", neg_wide_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
